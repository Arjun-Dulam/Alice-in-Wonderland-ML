\documentclass{article}

\usepackage{amsthm} 
\usepackage{amsmath}    
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}            
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}            
\usepackage{enumerate}  
\usepackage{dirtytalk}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{listings}
\usepackage{xcolor}

\title{\vspace{-3cm}Alice in Wonderland ML Notes}
\author{}
\date{}

\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}

\begin{document}
\maketitle

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red}
}
\setcounter{section}{1}
\section{Mathemacial Preliminaries}

\subsection{Linear Algebra}

\begin{itemize}
    \item A \tbf{tensor} $X$ is an n-dimensional array of elements of the same type. $X \sim (s_1,s_2,\cdot,s_n)$ denotes the shape of the tensor.
\end{itemize}

\subsubsection{Vector Operations}

\begin{itemize}
    \item A property of the dot product is that the maximum value of the dot product of two normalized vectors occurs when both vectors are the same. 
    \begin{itemize}
        \item When $\mbf{x}$, which represents the input, and $\mbf{w}$, which represents adaptable parameters, resonate, the dot product is maximized.
        \item This is called template matching. 
    \end{itemize} 
\end{itemize}

\subsubsection{Matrix Operations}

\begin{itemize}
     \item Given two matrices $\mbf{X}$ and $\mbf{Y}$, matrix multiplication is defined element wise as: $\mbf{Z}_{ij} = \mbf{X}_i \cdot \mbf{Y}_j$ 
        i.e. the element $(i,j)$ of the product is the dot product of the $i$-th row of $\mbf{X}$ and the $j$-th column of $\mbf{Y}$.
        \item The Hadamard method of multiplying matrices is element wise multiplication where each element of the resulting matrix $\mbf{Z}$ is given by $\mbf{Z}_{ij} = \mbf{X}_{ij} \cdot \mbf{Y}_{ij}$.
        \item The Hadamard multiplication method is used primarily to mask matrices i.e. setting some elements to zero or scaling operations. 
        \item The Hadamard multiplication method does not preserve linearity and cannot be used in operations where linearity is required. Additionally, it cannot be used in compositions of functions such as $f(g(x))$ because it operates element-wise rather than on the entire structure of the matrices.
        \item There are many operations that can be done element wise or with whole matrices. PyTorch has built in modules for both types of operations. 
\end{itemize}

\subsubsection{Higher-order Tensor Operations}

\begin{itemize}
    \item When in higher dimensions, most of the operations we are interested in are either batched variants matrix operations, or specific combinations of matrix operations and reduction operations. 
    \item Example: with two tensors $\mbf{X} \sim (n,a,b) \text{ and } \mbf{Y} \sim (n,b,c)$, the batched matrix multiplication is defined as $\mbf{Z} \sim (n,a,c)$ where $\mbf{Z}_{i} = \mbf{X}_i \cdot \mbf{Y}_i$.
\end{itemize}

\subsection{Gradients \& Jacobians}

\begin{itemize}
    \item Gradients play a pivotal role in optimization algorithms by providing semi-automatic mechanisms deriving from gradient descent. 
\end{itemize}

\subsubsection{Gradients and Directional Derivatives}

\begin{itemize}
    \item The gradient of a function is defined as:
    \[
    \nabla f(\mathbf{x}) = \partial f(\mathbf{x}) =
    \begin{bmatrix}
    \partial_{x_1} f(\mathbf{x}) \\
    \vdots \\
    \partial_{x_d} f(\mathbf{x})
    \end{bmatrix}
    \]
    \item The directional derivative is the dot product of the gradient and the direction vector:
    \[\nabla f(x) \cdot \mbf{v}\]
\end{itemize}

\subsubsection{Jacobians}

\begin{itemize}
    \item Let there be a function $f(x)$ that maps a vector input $\mbf{x} \sim (d)$ to a vector output $\mbf{y} \sim (c)$. To calculate the gradient for each output, we must create the \textbf{Jacobian} of $f$.
        
        \[\partial f(\mathbf{x}) = 
        \begin{bmatrix}
        \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_d} \\
        \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_d} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial y_c}{\partial x_1} & \frac{\partial y_c}{\partial x_2} & \cdots & \frac{\partial y_c}{\partial x_d}
        \end{bmatrix}
        \]
    \item Each column of the Jacobian corresponds to the gradient of $f(x)$ that maximizes a specific value within the output vector $\mbf{y}$.
    \item Each row of the Jacobian describes how the rate of change for the outputs changes with respect to a specific input.
    \item When $c$ is equal to 1, i.e. when there is only a single output parameter, the matrix simplifies to a single row vector which is the gradient of the function $f(x)$.
    \item When $c = 1 = d$, the Jacobian becomes the standard derivative of the function. 
    \item Jacobians inherit the properties of derivatives, including the fact that the Jacobian of a compositions of functions is now the matrix multiplication of the individual Jacobians. 
    \item For a point $x_0$, the best linear approximation to $f(x)$ is $f(\mbf{x}_0) + \partial f(\mbf{x}_0) \cdot (\mbf{x} - \mbf{x}_0)$. This is called Taylor's theorem.
    \item A code example:
\begin{lstlisting}
$ Generic mathematical function
f = lambda x: x**2 - 1.5*x

# Derivative 
df = lambda x: 2*x - 1.5

x = 0.5
f_linearized = lambda h: f(x) + df(x)*(h-x)

#Comparing approximation to actual function
print(f(x + 0.01)) # [Out] = -0.5049
print(f_linearized(x + 0.01)) # [Out] = -0.5050
\end{lstlisting}
\end{itemize}

\subsection{Numerical Optimization and Gradient Descent}
\begin{itemize}
    \item Consider the problem of trying to find the minimum of a function $f(x)$. Assuming the function has a single output \tbf{single-objective optimization}, we try to find a global minimum within an uncontrained domaine.
    \item It it possible to express the solution in closed-form (where there is a function to find the optimal $\mbf{x})$, but in general we must resort to iterative procedures. 
    \item Let's start with a random guess $\mbf{x_0}$ and for every iteration, we decompose the new position as the sum of the old position  + the magnitude of the step times the direction of the step:
    \[\mbf{x}_t = \mbf{x}_{t-1} + \eta_t \cdot \mbf{p}_t\]
    where $\eta_t$ is the length of the step and $\mbf{p}_t$ is the normalized direction vector.
    \item We call $\eta_t$ the \tbf{learning rate} and a direction $\mbf{p}_t$ such that $f(\mbf{x}_t) \leq f(\mbf{x}_{t-1})$ the \tbf{descent direction}.
    \item Selecting a descent direction for every iteration and being careful with choice of step size will allow us to converge to a local minimum.
    \item Given that $\mbf{p}_t$ is the descent direction, it is known that $D_{\mbf{p_t}} f(\mbf{x_{t-1}}) \leq 0$.
    \item Given that the directional derivative is the dot product of the gradient and the direction vector, we can conclude:
    \[D_{\mbf{p_t}} f(\mbf{x_{t-1}}) = \nabla f(x_{t-1}) \cdot \mbf{p}_t = ||\nabla f(x_{t-1})|| \cdot ||\mbf{p}_t|| \cdot \cos \alpha \]
    where $\alpha$ is the angle between the gradient and the descent direction. 
    \item The first term is a constant with respect to $\mbf{p_t}$, and $||\mbf{p_t}||$ can be assumed to be equal to 1 as it's a normalized direction vector. With this information, we can simplify the previous formula:
    \[D_{\mbf{p_t}} f(\mbf{x_{t-1}}) = ||\nabla f(x_{t-1})||  \cdot \cos \alpha \]
    \item The properties of consine result in it being negative when  $\frac{\pi}{2} < \alpha < \frac{3 \pi}{2}$, therefore any $\mbf{p_t}$ that forms an angle $a$ satisfying the previous inequality will be a descent direction.
    \item The \tbf{steepest descent direction} is the direction where $\mbf{p_t}$ forms an angle of $\pi$ with $\nabla f(\mbf{x_{t-1}})$ which is synonymous with $\mbf{p_t} = -\nabla f(\mbf{x_{t-1}})$.
    \item On an intutive level, this makes sense as the gradient points in the direction of greatest increase, so the negative of the gradient would point in the direction of greatest descrease. 
    \item The previous formula can be rewritten as:
    \[\mbf{x}_t = \mbf{x}_{t-1} - \eta _t \nabla f(\mbf{x}_{t-1})\]
    \item The step size doesn't matter all that much as long as the size is small enough for $f$ to reduce with each iteration. 
    
    \end{itemize}
    \subsubsection{Convergence of Gradient Descent}

    \begin{itemize}
        \item The formal definition for a local minimum of $f(x)$ is a point $\mbf{x}^+$ such that the following is true for some $\epsilon > 0$:
        \[f(\mbf{x}^+) \leq f(\mbf{x}) \hspace{.1cm} \forall \mbf{x} : ||\mbf{x} - \mbf{x}^+|| < \epsilon\]
        \item In other words, the function $f(\mbf{x})$ exists at a local minimum at a point $\mbf{x}^+$ if for some positive value $\epsilon$, 
        $f(\mbf{x}^+)$ is less than every point $\epsilon$ distance away from $\mbf{x}^+$. 
        \item By the definition of the local minimum, a function at some local minimum will only ever increase if it enters the neighborhood around the local minimum. Thus the 
        gradient at a local minimum is zero and the gradient around the local minimum is pointing upwards. 
        \item A \tbf{stationary point} of $f(\mbf{x})$ is a point $\mbf{x}^+$ such that $\nabla f(\mbf{x}^+) = 0$.
        \item Stationary points exist at all minima, maxima, and saddle points i.e. where $\nabla f(\mbf{x}) = 0$.
        \item Due to this, we can only guarantee that gradient descent will converge to a stationary point, not necessarily a local minimum. 
        \item Ideally, we would want to attain the \tbf{global minimum} of a function, the one (or possibly one of many) point(s) in the domain where $f(\mbf{x})$ attains its lowest possible value.
        \item For the sake of visualization, assume  $f(\mbf{x}) \in \mathbb{R}^3$. If the function assumes a parabolic shape, then every point in the domain will have a gradient pointing toward the global minimum.
        \item With the previous example, the topic of \tbf{convexity} comes up. A function $f(\mbf{x})$ is convex if for any two points $\mbf{x}_1$ and $\mbf{x}_2$, and $\alpha \in [0,1]$, we have:
        \[f(\underbrace{\alpha \mbf{x}_1 + (1-\alpha)\mbf{x}_2}_{\text{Interval from $\mbf{x}_1$ to $\mbf{x}_2$}}) \leq \underbrace{\alpha f(\mbf{x}_1) + (1 - \alpha) f(\mbf{x}_2)}_{\text{Line segment from $f(\mbf{x_1})$ to $f(\mbf{x}_2)$}}\]
        \item In words, a function is convex if the line segment connecting two points $f(\mbf{x}_1)$ and $f(\mbf{x}_2)$ is always greater than or equal to every single value on the function between $\mbf{x}_1$ and $\mbf{x}_2$. 
        \item A convex function simplified our task greatly for the following reasons:
        \begin{itemize}
            \item For a generic non-convex function, gradient descent will always converge onto a stationary point, not necessarily a local minimum.
            \item For a convex function, the stationary point is the global minimum.
            \item if the inequality earlier is satisfied in a strict way (\tbf{strict convexity}), then the global minimizer is guaranteed to be unique. 
        \end{itemize}
        \item Trying to find the global minimum is a non-convex problem with gradient descent is impossible because you must run the algorithm for an infinite amount of time to check the infinite amount of points from an infinite amount of initializations in the unconstrained domain. 
        \end{itemize}
        
        \subsubsection{Accelerating Gradient Descent}

        \begin{itemize}
            \item A problem with the gradient approach is that it only points to the greatest descent direction in an extemely small neighborhood around the current point. 
            This can lead to very noisy updates and slow convergence.
            \item To smooth out the erratic changes in descent direction, we can make the direction of the current step to affect the direction of the next step. Such a method is called \tbf{momentum}:
            \begin{align*}
                \mbf{g}_t &= - \underbrace{\eta_t \nabla f(\mbf{x}_{t-1})}_{\text{gradient descent}} + \underbrace{\lambda \mbf{g}_{t-1}}_{\text{momentum}} \\
                \mbf{x}_t &= \mbf{x}_{t-1} + \mbf{g}_t
            \end{align*}
            where we initialize $\mbf{g}_0 = 0$ and $\lambda$ is a parameter that determines how much the previous term is dampened.
            \item Expanding two terms:
            \begin{align*}
                \mbf{g}_t &= - \eta \nabla f(\mbf{x}_{t-1}) + \lambda (-\eta_t \nabla f(\mbf{x}_{t-2})+\lambda \mbf{g}_{t-2}) \\
                &= -\eta_t \nabla f(\mbf{x}_{t-1}) - \lambda \eta_t \nabla f(\mbf{x}_{t-2}) + \lambda^2 \mbf{g}_{t-2}
            \end{align*}
            \item The momentum method has been shown to accelerate training by smoothing the optimization path. Also, modifying the step size depending on the gradient is another method. 
            Usually, the step size and the gradient are inversely proportional.
        \end{itemize}


\section{Datasets and Losses}

\subsection{What is a Dataset?}

\begin{itemize}
    \item A supervised dataset $S_n$ of size $n$ is a set of $n$ pairs
    \[ S_n = \{(x_i, y_i)\}_{i=1}^n \]
    where each $(x_i, y_i)$ is an example of an input-output relationship we want to model. We further assume that each example is an identically and independently distributed
    draw from some unknown (and unknowable) probability distribution $p(x, y)$.
    \item A sample being \tbf{identicially distributed} means that we are trying to track something that is sufficiently stable in terms of change over time. 
    Take the task of identifying car models from photos.
    Since car models change over time, we will not be able to to have an identical distribution of car models in a dataset with large discrepancies on the time when the data was collected
    \item A sample being \tbf{independently distributed} means that there is no inherent bias in our training data. 
    This condition would not be satisfied if we exclusively collected data from outside a Tesla dealership. 
\end{itemize}

\subsubsection{Variants of Supervised Learning}

\begin{itemize}
    \item In datasets with not enough targets $y_i$, we can use \tbf{unsupervised learning}. Typical applications of unsupervised learning are \tbf{clustering algorithms}, 
    where points between clusters are similar and points within clusters are dissimilar. An example of this would be grouping together simlar news articles in terms of topics. 
    Another example is a \tbf{retrieval} system, where we retrieve the most similar elements to the user's query.
    \item Unsupervised learning in itself is not ideal for image classification, because the slightest modifiction to an image can lead to millions of pixels being changed. 
    A model that's already optimized for image classification, a \tbf{pre-trained} model, can be used to extract features from the images.
    \item The states of this model can be interpreted as vectors in a higher-dimensional space. These vectors can be mapped and used to train a classifier.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=4in]{../miscImages/preTrain.png} 
        \caption{High level overview of using pre-trained model}
        \label{fig:example}
    \end{figure}

    \item \tbf{Self-supervised learning} is a variant of un-supervised learning where the model is trained to find some supervised objective from a unsupervised dataset. 
    An example of would be this: A model is given a large piece of text. The model removes a specific part from each sentence in the text and tries to guess the removed part. 
    Comparing its guess to the actual removed part is the supervised objective, the model continuously learns from comparing its guess to the removed part.

    \item There are three ways of using trained models:
    
    \begin{itemize}
        \item \tbf{Zero-Shot Learning}: An trained model is given a task it has not training on. It is given no extra data on the task, and must rely on its previous training. 
        \item \tbf{Few-Shot Prompting}: A trained model is given a task it has not training on. It is given a few examples of the task and uses its existing knowledge to make a new inference.
        \item \tbf{Fine-Tuning}: A trained model is furthet trained on a specific task. This allows it to adapt to the specific needs of the task while using its prior existing knowledge.
    \end{itemize}

    \item When fine tuning, the model can have all of its parameters changes, or change/add a few parametets. The latter is called \tbf{parameter efficient fine-tuning}.   
    \item \tbf{Semi-supervised learning} is a variant of supervised learning where the model is trained on a dataset with a small number of labeled examples and a large number of unlabeled examples.

\end{itemize}

\subsection{Loss Functions}

\begin{itemize}
    \item Given a desried targey $y$ and the predicted value $\hat{y} = f(x)$ from a model $f$, 
    a \tbf{loss function} $l(y, \hat{y}) \in \mathbb{R}$ is a scalar, differentiable function whose value correlates with the performance of the model.
    The performance of the model is measured by the minimization of the loss function i.e. $l(y, \hat{y}_1) < l(y, \hat{y}_2)$ implies that $\hat{y}_1$ is a better prediction than $\hat{y}_2$
    wrt the target $y$.
    \item The loss function's scalar and differentiable properties allow it to be minimized with a gradient descent algorithm. 
    \item Given a dataset $S_n = \{(x_i, y_i)\}$, and a loss function $l(.,.)$, the optimization task at hand is to minimum average loss on the dataset by any possible differentiable model $f$:
    \[f^* = \arg _f\min \underbrace{\frac{1}{n}\sum_{i=1}^{n}}_{\text{average}}\underbrace{l(y_i, f(x_i))}_{\text{loss value}}\]
    \item Essentially, we're trying to find the best model that minimizes the loss function. 
    We do this by getting the average of the losses for every single prediction a model makes on a certain dataset.
    We compare this loss with the average loss of other models on the same dataset, the model with the lowest average loss is best at making predictions for inputs in the dataset. 
    \item This is called \tbf{empirircal risk minimization} (risk is generic synonym for loss).
    \item Models can be parameterized by a set of tensors $w$ (called parameters of the model), 
    and minimization is done by searching for the optimal value of these parameters via numerical optimization, denoted by $f(x,w)$.
    \item $f(x,w)$ represents the prediction when giving an input $x$ into a model with parameters $w$.
    \item Hence, the optimization task can be rewritten as:
    \[w^* = \arg _w \min \frac{1}{n} \sum^n_{i=1}l(y_1, f(x_1,w))\]
    \item In this context, we determine the optimal parameters for a specific model that minimizes the average loss on the dataset. 
\end{itemize}
\paragraph{On the differentiability of the loss function}
\begin{itemize}
    \item Consider a model $f$ that outputs a $y \in \{-1,+1\}$ where the true target can only take on two values : -1 and +1.
    \item We can equate the two possible correct outputs with the sign of $f$, denoted $\operatorname{sign(f(x))}$.
    \item One possible loss function is the \tbf{0/1 loss}:
    \[
    l(y, \hat{y}) = 
    \begin{cases}
        0 \text{ if } \operatorname{sign}(\hat{y}) = y \\
        1 \text{ otherwise }
    \end{cases}
    \]
    \text{This is not differentiable, so the gradient descent algorithm would not work to minimize it.}
    \item Another one is \tbf{margin} $y \hat{y}$, which will be positive if the prediction is correct and negative otherwise. 
    This is preferable as is is continuously differentiable. 
    \item The \tbf{hinge loss} function $l(x,y) = max(0,1-y\hat{y})$ is a continuous and differentiable loss function used to train support-vector models.
\end{itemize}

\subsubsection{Expected Risk and Overfitting}

\begin{itemize}
    \item The loss function can be completely minimized if we only respond to inputs already within a dataset, 
    however the objective is to minimize the loss function for all possible inputs.
    \item The \tbf{expected risk} given a probability distribution $p(x,y)$ and a loss function $l$ is defined as:
    \[\text{ER}[f] = \mathbb{E}_{p(x,y)}[l(y,f(x))]\]
    \item This expression shows the expected risk of a model $f$, denoted ER$[f]$, for all possible input-output pairs $(x,y)$ on a probability distribution. 
    \item The equation in unfeasiable to calculate, so the \tbf{empirical} risk is an estimate of the expected risk with a given dataset.
    \item The difference in loss between the expected and empirircal risk is called \tbf{generalization gap}. 
    \item A overly specific model based on memorization will have a large generalization gap, as it will overfit to the training data, 
    but does not respond well to new data.
    \item Generalization can be tested by using a separate \tbf{test dataset} that the model has not seen before.
\end{itemize}

\subsubsection{Selecting Valid Loss Functions}

\begin{itemize}
    \item Assuming our examples come from a distribution $p(x,y)$, we can decompose it as $p(x,y) = p(x) \cdot p(y | x)$.
    \item The function $f(x)$ is used to predict $p(y|x)$, that is, the chance that the model will give the correct output $y$ given the input $x$.
    \item Approximating $p(y|x)$ with a function $f(x)$ is viable if we assume that the probability mass is mostly centered
    around a single point $y$ i.e. there are not multiple points $y_1, y_2, \cdots y_n$ that are likely to be be the output.
    \item However, if we move away from the previous definition of $f(x)$ and instead think of $f(x)$ as
    a parameterization of the chances of the different outputs $y_1, y_2, \cdots, y_n$ given $x$, we can represent $f(x)$ as:
    \[f(x) = [p(y_1 | x) , p(y_2 | x), p(y_3 | x)]\]
    \item Similarly, we also define $\mbf{y} \sim \operatorname{Binary}(n)$ where $\mbf{y}$ is a one-hot encoded vector that contains a 1 at the correct output's place.
    \item Thus, we can write: 
    \[p(\mbf{y} | f(x) = \prod^3_{i = 1}f_i(x)^{y_1}\]
    \item This chain of logic can be shown via an example:
    Assume there is a model, given an input $x$, outputs a $y \in \{1,2,3\}$. The model's chances of giving these outputs are, respectively, $f(x) = [0.2, 0.5, 0.3]$.
    Assume that the correct output is $y = 2$. Thus, the correct representation of $\mbf{y} \sim 3$ is $[0,1,0]$.
    Following the expression above, $p(\mbf{y} | f(x))$ can be calculated as:
    \begin{align*}
        p(\mbf{y} | f(x)) &= f_1(x)^{y_1} \cdot f_2(x)^{y_2} \cdot f_3(x)^{y_3} \\
        &= 0.2^0 \cdot 0.5^1 \cdot 0.3^0 \\
        &= 0.5 
    \end{align*}
    Thus, the probability of the model giving the correct output is 0.5.

    \item Our formula does not directly give the probability for the output, but instead gives a probability distribution over the possible outputs, with the correct one being one of them.
\end{itemize}

\subsubsection{Maximum Likelihood}

\begin{itemize}
    \item Assume that the samples are independent and identically distributed (i.i.d) from a probability distribution $p(x,y)$. Recall that the probability distribution $p(x,y)$ describes the probability of observing the input $x$ and $y$ together. Hence, the probability assigned to the dataset itself by a specific choice $f$ of function is given by the product of each sample in the dataset:
    \[p(\mathcal{S}_n|f) = \prod^{\pi}_{i=1}p(y_i|f(x_i))\]
    \item This formula defines the probability that a given model $f$ will provide outputs $y_1, y_2, \cdots, y_n$ that are the same as the outputs in the dataset $\mathcal{S}_n$ given the inputs $x_1, x_2, \cdots, x_n$.
    \item The quantity $p(\mathcal{S}_n | f)$ is called the \tbf{likelihood} of the set. This quantity is to be maximized up to a certain extent, where the model's outputs do not deviate greatly from the dataset's outputs but also where the model is also not overfitted with the dataset (and thus struggling with inputs outside of the dataset).
    \item Given a dataset $\mathcal{S}_n=\{\{x_i, y_i\}\}$ and a family of probability distributions $p(y|f(x)$ parameterized by $f(x)$, the maximum likelihood solution is given by:
    \[f^* = \arg _f \max \prod ^\pi _{i=1} p(y_i|f(x_i))\]
    Essentially, this formula provides the maximum likelihood across all models by providing the likelihood of the best function $f^*$.
    \item Switching to a minimization problem, we get:
    \[
    \arg _f \max \left\{ \log \prod ^n _{i=1} p(y_i|f(x_i))\right\} = \arg _f \min \left\{\sum^n_{i=1} -\log(p(y_i|f(x_i))\right\}
    \]
    A few notes on this equation:
    \begin{itemize}
        \item We use the log function because multiplying probabilities $<1$ can result in a very small product, leading to underflow when storing into memory. Using log helps alleviate this issue.
        \item Reminder: the log of a product is equivalent to the sum of the logs of items being multiplied:
        \[\log (ab)= \log(a) + \log(b)\]
        Converting this to an addition problem results in reduced computing demand.
        \item Adding the negative sign on the right-hand side of the equations turns this into a minimization problem which has been covered earlier.
    \end{itemize}
    \end{itemize}
    \subsection{Bayesian Learning}
    \begin{itemize}
        \item When designing a probability function $p(y|f(X))$ instead of $f(x)$, we can handle situations where the model might give different possible outputs. 
        \item This procedure however only looks an one singular function with a specific parameterization. What if we had multiple functions with different parameterization that all provide good outputs? It would be wasteful to rely on one singular function instead of relying on multiple and choosing the best model for an input based on its output.
        \item We can achieve this by defining a \tbf{prior probability distribution} $p(f)$ over all possible functions. Recall that $f$ is a model with a certain set of parameters.
        \begin{itemize}
            \item Recall that a \tbf{Bayesian prior} is a initial belief on what a parameter might be.
        \end{itemize}
        \item Functions with smaller norms are preferred so setting up an inverse relationship with the parameters' norm would be useful in creating our priors:
        \[p(f) \propto \frac{1}{||f||}\]
        \item Once a dataset is observed, the probability over $f$ shifts depending on the prior and the likelihood, and the update is given by \tbf{Bayes' theorem}.
        \[\underbrace{p(f|\mathcal{S}_n)}_{posterior}=\frac{p(\mathcal{S}_n|f))\cdot \overbrace{p(f)}^{prior}}{p(\mathcal{S}_n)}\]

        The term $p(f|\mathcal{S})$ is called the \tbf{posterior distribution function}, while the term $p(\mathcal{S}_n)$ is called the \tbf{evidence} and normalizes the right-hand side of the equation.

        \item Given an input $x$, we can make a prediction by averaging all possible models based on their posterior's weight:
        \[p(y|x) = \int _f p(y|f(x)) \cdot p(f|\mathcal{S}_n) \approx \frac{1}{k} \sum ^k _{i=1} p(y|f_i(x)) \cdot p(f_i|\mathcal{S}_n)\]
        Here, we take the average of all models when we take the summation and divide by the amount of models on the right hand side.
        \item If we are solely interested in maximizing the posterior term, we can discard the evidence term as that remains relatively constant. As a result, we can choose only to focus on the terms in the numerator:
        
        \[f^* = \arg \max p(\mathcal{S}_n | f)p(f) = \arg _f \max \left\{\underbrace{\log p(\mathcal{S}_n|f)}_{likelihood} + \underbrace{\log p(f)}_{regularization} \right\} \]
        This is the \tbf{maximum a posteriori} (MAP) solution. 
        \item If all functions have the same weight a priori, then the proble is reduced down to a maximum likelihood solution. The regularizer term pushes the solution toward the basin of attraction defined by the prior distribution.
    \end{itemize}

    \section{Linear Models}

    \subsection{Least-Squares Regression}

    \subsubsection{Problem setup}
        
    Recall that a supervised learning problem can be defined by choosing the input type $x$, the output type $y$, the model $f$, and the loss function $l$.
    
    \begin{itemize}
        \item The input is a vector $\mbf{x} \sim (c)$, corresponding to $c$ number of features in the input.
        \item The output is a single real value $\in \mathbb{R}$. 
        \begin{itemize}
            \item If $y$ can take any real value, this is a \tbf{regression} task/
            \item If $y$ can only take one out of $m$ possible values, this is a \tbf{classification} task.
            \item If $y$ can only take one of two possible values, this is a \tbf{binary classification} task.
        \end{itemize}
        \item We assume $f$ is a linear model.
        \item We assume that:
        \begin{itemize}
            \item $n \rightarrow$ size of the database
            \item $c \rightarrow$ features of input
            \item $m \rightarrow$ classes of outputs
        \end{itemize}
    \end{itemize}
    \subsubsection{Regression Losses: The Square Loss and Variants}
\end{document}
