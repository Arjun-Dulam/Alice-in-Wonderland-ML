\documentclass{article}

\usepackage{amsthm} 
\usepackage{amsmath}    
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}            
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}            
\usepackage{enumerate}  
\usepackage{dirtytalk}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{mathtools}


\title{\vspace{-3cm}Alice in Wonderland ML Notes}
\author{}
\date{}

\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}


\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red}
    }
    
\hypersetup{
    colorlinks=true,       % Makes links colored
    linkcolor=blue,        % Color of internal links (e.g., table of contents)
    urlcolor=blue
}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm} % Adjust the vertical spacing

    {\Huge\textbf{Alice in Wonderland ML Notes}\par} % Main title in bold and large font
    \vspace{1.5cm} % Space below the title

    {\Large Arjun Dulam\par} % Your name
    \vspace{1.5cm} % Space below the author

    {\large \href{https://github.com/Arjun-Dulam/Alice-in-Wonderland-ML}{Github Repository Link} \par} % Subtitle (optional)
    \vfill % Pushes the following content to the bottom of the page
\end{titlepage}
        
\tableofcontents
\newpage

\setcounter{section}{1}
\section{Mathemacial Preliminaries}

\subsection{Linear Algebra}

\begin{itemize}
    \item A \tbf{tensor} $X$ is an n-dimensional array of elements of the same type. $X \sim (s_1,s_2,\cdot,s_n)$ denotes the shape of the tensor.
\end{itemize}

\subsubsection{Vector Operations}

\begin{itemize}
    \item A property of the dot product is that the maximum value of the dot product of two normalized vectors occurs when both vectors are the same. 
    \begin{itemize}
        \item When $\mbf{x}$, which represents the input, and $\mbf{w}$, which represents adaptable parameters, resonate, the dot product is maximized.
        \item This is called template matching. 
    \end{itemize} 
\end{itemize}

\subsubsection{Matrix Operations}

\begin{itemize}
     \item Given two matrices $\mbf{X}$ and $\mbf{Y}$, matrix multiplication is defined element wise as: $\mbf{Z}_{ij} = \mbf{X}_i \cdot \mbf{Y}_j$ 
        i.e. the element $(i,j)$ of the product is the dot product of the $i$-th row of $\mbf{X}$ and the $j$-th column of $\mbf{Y}$.
        \item The Hadamard method of multiplying matrices is element wise multiplication where each element of the resulting matrix $\mbf{Z}$ is given by $\mbf{Z}_{ij} = \mbf{X}_{ij} \cdot \mbf{Y}_{ij}$.
        \item The Hadamard multiplication method is used primarily to mask matrices i.e. setting some elements to zero or scaling operations. 
        \item The Hadamard multiplication method does not preserve linearity and cannot be used in operations where linearity is required. Additionally, it cannot be used in compositions of functions such as $f(g(x))$ because it operates element-wise rather than on the entire structure of the matrices.
        \item There are many operations that can be done element wise or with whole matrices. PyTorch has built in modules for both types of operations. 
\end{itemize}

\subsubsection{Higher-order Tensor Operations}

\begin{itemize}
    \item When in higher dimensions, most of the operations we are interested in are either batched variants matrix operations, or specific combinations of matrix operations and reduction operations. 
    \item Example: with two tensors $\mbf{X} \sim (n,a,b) \text{ and } \mbf{Y} \sim (n,b,c)$, the batched matrix multiplication is defined as $\mbf{Z} \sim (n,a,c)$ where $\mbf{Z}_{i} = \mbf{X}_i \cdot \mbf{Y}_i$.
\end{itemize}

\subsection{Gradients \& Jacobians}

\begin{itemize}
    \item Gradients play a pivotal role in optimization algorithms by providing semi-automatic mechanisms deriving from gradient descent. 
\end{itemize}

\subsubsection{Gradients and Directional Derivatives}

\begin{itemize}
    \item The gradient of a function is defined as:
    \[
    \nabla f(\mathbf{x}) = \partial f(\mathbf{x}) =
    \begin{bmatrix}
    \partial_{x_1} f(\mathbf{x}) \\
    \vdots \\
    \partial_{x_d} f(\mathbf{x})
    \end{bmatrix}
    \]
    \item The directional derivative is the dot product of the gradient and the direction vector:
    \[\nabla f(x) \cdot \mbf{v}\]
\end{itemize}

\subsubsection{Jacobians}

\begin{itemize}
    \item Let there be a function $f(x)$ that maps a vector input $\mbf{x} \sim (d)$ to a vector output $\mbf{y} \sim (c)$. To calculate the gradient for each output, we must create the \textbf{Jacobian} of $f$.
        
        \[\partial f(\mathbf{x}) = 
        \begin{bmatrix}
        \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_d} \\
        \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_d} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial y_c}{\partial x_1} & \frac{\partial y_c}{\partial x_2} & \cdots & \frac{\partial y_c}{\partial x_d}
        \end{bmatrix}
        \]
    \item Each column of the Jacobian corresponds to the gradient of $f(x)$ that maximizes a specific value within the output vector $\mbf{y}$.
    \item Each row of the Jacobian describes how the rate of change for the outputs changes with respect to a specific input.
    \item When $c$ is equal to 1, i.e. when there is only a single output parameter, the matrix simplifies to a single row vector which is the gradient of the function $f(x)$.
    \item When $c = 1 = d$, the Jacobian becomes the standard derivative of the function. 
    \item Jacobians inherit the properties of derivatives, including the fact that the Jacobian of a compositions of functions is now the matrix multiplication of the individual Jacobians. 
    \item For a point $x_0$, the best linear approximation to $f(x)$ is $f(\mbf{x}_0) + \partial f(\mbf{x}_0) \cdot (\mbf{x} - \mbf{x}_0)$. This is called Taylor's theorem.
    \item A code example:
\begin{lstlisting}
# Generic mathematical function
f = lambda x: x**2 - 1.5*x

# Derivative 
df = lambda x: 2*x - 1.5

x = 0.5
f_linearized = lambda h: f(x) + df(x)*(h-x)

#Comparing approximation to actual function
print(f(x + 0.01)) # [Out] = -0.5049
print(f_linearized(x + 0.01)) # [Out] = -0.5050
\end{lstlisting}
\end{itemize}

\subsection{Numerical Optimization and Gradient Descent}
\begin{itemize}
    \item Consider the problem of trying to find the minimum of a function $f(x)$. Assuming the function has a single output \tbf{single-objective optimization}, we try to find a global minimum within an uncontrained domaine.
    \item It it possible to express the solution in closed-form (where there is a function to find the optimal $\mbf{x})$, but in general we must resort to iterative procedures. 
    \item Let's start with a random guess $\mbf{x_0}$ and for every iteration, we decompose the new position as the sum of the old position  + the magnitude of the step times the direction of the step:
    \[\mbf{x}_t = \mbf{x}_{t-1} + \eta_t \cdot \mbf{p}_t\]
    where $\eta_t$ is the length of the step and $\mbf{p}_t$ is the normalized direction vector.
    \item We call $\eta_t$ the \tbf{learning rate} and a direction $\mbf{p}_t$ such that $f(\mbf{x}_t) \leq f(\mbf{x}_{t-1})$ the \tbf{descent direction}.
    \item Selecting a descent direction for every iteration and being careful with choice of step size will allow us to converge to a local minimum.
    \item Given that $\mbf{p}_t$ is the descent direction, it is known that $D_{\mbf{p_t}} f(\mbf{x_{t-1}}) \leq 0$.
    \item Given that the directional derivative is the dot product of the gradient and the direction vector, we can conclude:
    \[D_{\mbf{p_t}} f(\mbf{x_{t-1}}) = \nabla f(x_{t-1}) \cdot \mbf{p}_t = ||\nabla f(x_{t-1})|| \cdot ||\mbf{p}_t|| \cdot \cos \alpha \]
    where $\alpha$ is the angle between the gradient and the descent direction. 
    \item The first term is a constant with respect to $\mbf{p_t}$, and $||\mbf{p_t}||$ can be assumed to be equal to 1 as it's a normalized direction vector. With this information, we can simplify the previous formula:
    \[D_{\mbf{p_t}} f(\mbf{x_{t-1}}) = ||\nabla f(x_{t-1})||  \cdot \cos \alpha \]
    \item The properties of consine result in it being negative when  $\frac{\pi}{2} < \alpha < \frac{3 \pi}{2}$, therefore any $\mbf{p_t}$ that forms an angle $a$ satisfying the previous inequality will be a descent direction.
    \item The \tbf{steepest descent direction} is the direction where $\mbf{p_t}$ forms an angle of $\pi$ with $\nabla f(\mbf{x_{t-1}})$ which is synonymous with $\mbf{p_t} = -\nabla f(\mbf{x_{t-1}})$.
    \item On an intutive level, this makes sense as the gradient points in the direction of greatest increase, so the negative of the gradient would point in the direction of greatest descrease. 
    \item The previous formula can be rewritten as:
    \[\mbf{x}_t = \mbf{x}_{t-1} - \eta _t \nabla f(\mbf{x}_{t-1})\]
    \item The step size doesn't matter all that much as long as the size is small enough for $f$ to reduce with each iteration. 
    
    \end{itemize}
    \subsubsection{Convergence of Gradient Descent}

    \begin{itemize}
        \item The formal definition for a local minimum of $f(x)$ is a point $\mbf{x}^+$ such that the following is true for some $\epsilon > 0$:
        \[f(\mbf{x}^+) \leq f(\mbf{x}) \hspace{.1cm} \forall \mbf{x} : ||\mbf{x} - \mbf{x}^+|| < \epsilon\]
        \item In other words, the function $f(\mbf{x})$ exists at a local minimum at a point $\mbf{x}^+$ if for some positive value $\epsilon$, 
        $f(\mbf{x}^+)$ is less than every point $\epsilon$ distance away from $\mbf{x}^+$. 
        \item By the definition of the local minimum, a function at some local minimum will only ever increase if it enters the neighborhood around the local minimum. Thus the 
        gradient at a local minimum is zero and the gradient around the local minimum is pointing upwards. 
        \item A \tbf{stationary point} of $f(\mbf{x})$ is a point $\mbf{x}^+$ such that $\nabla f(\mbf{x}^+) = 0$.
        \item Stationary points exist at all minima, maxima, and saddle points i.e. where $\nabla f(\mbf{x}) = 0$.
        \item Due to this, we can only guarantee that gradient descent will converge to a stationary point, not necessarily a local minimum. 
        \item Ideally, we would want to attain the \tbf{global minimum} of a function, the one (or possibly one of many) point(s) in the domain where $f(\mbf{x})$ attains its lowest possible value.
        \item For the sake of visualization, assume  $f(\mbf{x}) \in \mathbb{R}^3$. If the function assumes a parabolic shape, then every point in the domain will have a gradient pointing toward the global minimum.
        \item With the previous example, the topic of \tbf{convexity} comes up. A function $f(\mbf{x})$ is convex if for any two points $\mbf{x}_1$ and $\mbf{x}_2$, and $\alpha \in [0,1]$, we have:
        \[f(\underbrace{\alpha \mbf{x}_1 + (1-\alpha)\mbf{x}_2}_{\text{Interval from $\mbf{x}_1$ to $\mbf{x}_2$}}) \leq \underbrace{\alpha f(\mbf{x}_1) + (1 - \alpha) f(\mbf{x}_2)}_{\text{Line segment from $f(\mbf{x_1})$ to $f(\mbf{x}_2)$}}\]
        \item In words, a function is convex if the line segment connecting two points $f(\mbf{x}_1)$ and $f(\mbf{x}_2)$ is always greater than or equal to every single value on the function between $\mbf{x}_1$ and $\mbf{x}_2$. 
        \item A convex function simplified our task greatly for the following reasons:
        \begin{itemize}
            \item For a generic non-convex function, gradient descent will always converge onto a stationary point, not necessarily a local minimum.
            \item For a convex function, the stationary point is the global minimum.
            \item if the inequality earlier is satisfied in a strict way (\tbf{strict convexity}), then the global minimizer is guaranteed to be unique. 
        \end{itemize}
        \item Trying to find the global minimum is a non-convex problem with gradient descent is impossible because you must run the algorithm for an infinite amount of time to check the infinite amount of points from an infinite amount of initializations in the unconstrained domain. 
        \end{itemize}
        
        \subsubsection{Accelerating Gradient Descent}

        \begin{itemize}
            \item A problem with the gradient approach is that it only points to the greatest descent direction in an extemely small neighborhood around the current point. 
            This can lead to very noisy updates and slow convergence.
            \item To smooth out the erratic changes in descent direction, we can make the direction of the current step to affect the direction of the next step. Such a method is called \tbf{momentum}:
            \begin{align*}
                \mbf{g}_t &= - \underbrace{\eta_t \nabla f(\mbf{x}_{t-1})}_{\text{gradient descent}} + \underbrace{\lambda \mbf{g}_{t-1}}_{\text{momentum}} \\
                \mbf{x}_t &= \mbf{x}_{t-1} + \mbf{g}_t
            \end{align*}
            where we initialize $\mbf{g}_0 = 0$ and $\lambda$ is a parameter that determines how much the previous term is dampened.
            \item Expanding two terms:
            \begin{align*}
                \mbf{g}_t &= - \eta \nabla f(\mbf{x}_{t-1}) + \lambda (-\eta_t \nabla f(\mbf{x}_{t-2})+\lambda \mbf{g}_{t-2}) \\
                &= -\eta_t \nabla f(\mbf{x}_{t-1}) - \lambda \eta_t \nabla f(\mbf{x}_{t-2}) + \lambda^2 \mbf{g}_{t-2}
            \end{align*}
            \item The momentum method has been shown to accelerate training by smoothing the optimization path. Also, modifying the step size depending on the gradient is another method. 
            Usually, the step size and the gradient are inversely proportional.
        \end{itemize}


\section{Datasets and Losses}

\subsection{What is a Dataset?}

\begin{itemize}
    \item A supervised dataset $S_n$ of size $n$ is a set of $n$ pairs
    \[ S_n = \{(x_i, y_i)\}_{i=1}^n \]
    where each $(x_i, y_i)$ is an example of an input-output relationship we want to model. We further assume that each example is an identically and independently distributed
    draw from some unknown (and unknowable) probability distribution $p(x, y)$.
    \item A sample being \tbf{identicially distributed} means that we are trying to track something that is sufficiently stable in terms of change over time. 
    Take the task of identifying car models from photos.
    Since car models change over time, we will not be able to to have an identical distribution of car models in a dataset with large discrepancies on the time when the data was collected
    \item A sample being \tbf{independently distributed} means that there is no inherent bias in our training data. 
    This condition would not be satisfied if we exclusively collected data from outside a Tesla dealership. 
\end{itemize}

\subsubsection{Variants of Supervised Learning}

\begin{itemize}
    \item In datasets with not enough targets $y_i$, we can use \tbf{unsupervised learning}. Typical applications of unsupervised learning are \tbf{clustering algorithms}, 
    where points between clusters are similar and points within clusters are dissimilar. An example of this would be grouping together simlar news articles in terms of topics. 
    Another example is a \tbf{retrieval} system, where we retrieve the most similar elements to the user's query.
    \item Unsupervised learning in itself is not ideal for image classification, because the slightest modifiction to an image can lead to millions of pixels being changed. 
    A model that's already optimized for image classification, a \tbf{pre-trained} model, can be used to extract features from the images.
    \item The states of this model can be interpreted as vectors in a higher-dimensional space. These vectors can be mapped and used to train a classifier.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=4in]{../miscImages/preTrain.png} 
        \caption{High level overview of using pre-trained model}
        \label{fig:example}
    \end{figure}

    \item \tbf{Self-supervised learning} is a variant of un-supervised learning where the model is trained to find some supervised objective from a unsupervised dataset. 
    An example of would be this: A model is given a large piece of text. The model removes a specific part from each sentence in the text and tries to guess the removed part. 
    Comparing its guess to the actual removed part is the supervised objective, the model continuously learns from comparing its guess to the removed part.

    \item There are three ways of using trained models:
    
    \begin{itemize}
        \item \tbf{Zero-Shot Learning}: An trained model is given a task it has not training on. It is given no extra data on the task, and must rely on its previous training. 
        \item \tbf{Few-Shot Prompting}: A trained model is given a task it has not training on. It is given a few examples of the task and uses its existing knowledge to make a new inference.
        \item \tbf{Fine-Tuning}: A trained model is furthet trained on a specific task. This allows it to adapt to the specific needs of the task while using its prior existing knowledge.
    \end{itemize}

    \item When fine tuning, the model can have all of its parameters changes, or change/add a few parametets. The latter is called \tbf{parameter efficient fine-tuning}.   
    \item \tbf{Semi-supervised learning} is a variant of supervised learning where the model is trained on a dataset with a small number of labeled examples and a large number of unlabeled examples.

\end{itemize}

\subsection{Loss Functions}

\begin{itemize}
    \item Given a desried targey $y$ and the predicted value $\hat{y} = f(x)$ from a model $f$, 
    a \tbf{loss function} $l(y, \hat{y}) \in \mathbb{R}$ is a scalar, differentiable function whose value correlates with the performance of the model.
    The performance of the model is measured by the minimization of the loss function i.e. $l(y, \hat{y}_1) < l(y, \hat{y}_2)$ implies that $\hat{y}_1$ is a better prediction than $\hat{y}_2$
    wrt the target $y$.
    \item The loss function's scalar and differentiable properties allow it to be minimized with a gradient descent algorithm. 
    \item Given a dataset $S_n = \{(x_i, y_i)\}$, and a loss function $l(.,.)$, the optimization task at hand is to minimum average loss on the dataset by any possible differentiable model $f$:
    \[f^* = \arg _f\min \underbrace{\frac{1}{n}\sum_{i=1}^{n}}_{\text{average}}\underbrace{l(y_i, f(x_i))}_{\text{loss value}}\]
    \item Essentially, we're trying to find the best model that minimizes the loss function. 
    We do this by getting the average of the losses for every single prediction a model makes on a certain dataset.
    We compare this loss with the average loss of other models on the same dataset, the model with the lowest average loss is best at making predictions for inputs in the dataset. 
    \item This is called \tbf{empirircal risk minimization} (risk is generic synonym for loss).
    \item Models can be parameterized by a set of tensors $w$ (called parameters of the model), 
    and minimization is done by searching for the optimal value of these parameters via numerical optimization, denoted by $f(x,w)$.
    \item $f(x,w)$ represents the prediction when giving an input $x$ into a model with parameters $w$.
    \item Hence, the optimization task can be rewritten as:
    \[w^* = \arg _w \min \frac{1}{n} \sum^n_{i=1}l(y_1, f(x_1,w))\]
    \item In this context, we determine the optimal parameters for a specific model that minimizes the average loss on the dataset. 
\end{itemize}
\paragraph{On the differentiability of the loss function}
\begin{itemize}
    \item Consider a model $f$ that outputs a $y \in \{-1,+1\}$ where the true target can only take on two values : -1 and +1.
    \item We can equate the two possible correct outputs with the sign of $f$, denoted $\operatorname{sign(f(x))}$.
    \item One possible loss function is the \tbf{0/1 loss}:
    \[
    l(y, \hat{y}) = 
    \begin{cases}
        0 \text{ if } \operatorname{sign}(\hat{y}) = y \\
        1 \text{ otherwise }
    \end{cases}
    \]
    \text{This is not differentiable, so the gradient descent algorithm would not work to minimize it.}
    \item Another one is \tbf{margin} $y \hat{y}$, which will be positive if the prediction is correct and negative otherwise. 
    This is preferable as is is continuously differentiable. 
    \item The \tbf{hinge loss} function $l(x,y) = max(0,1-y\hat{y})$ is a continuous and differentiable loss function used to train support-vector models.
\end{itemize}

\subsubsection{Expected Risk and Overfitting}

\begin{itemize}
    \item The loss function can be completely minimized if we only respond to inputs already within a dataset, 
    however the objective is to minimize the loss function for all possible inputs.
    \item The \tbf{expected risk} given a probability distribution $p(x,y)$ and a loss function $l$ is defined as:
    \[\text{ER}[f] = \mathbb{E}_{p(x,y)}[l(y,f(x))]\]
    \item This expression shows the expected risk of a model $f$, denoted ER$[f]$, for all possible input-output pairs $(x,y)$ on a probability distribution. 
    \item The equation in unfeasiable to calculate, so the \tbf{empirical} risk is an estimate of the expected risk with a given dataset.
    \item The difference in loss between the expected and empirircal risk is called \tbf{generalization gap}. 
    \item A overly specific model based on memorization will have a large generalization gap, as it will overfit to the training data, 
    but does not respond well to new data.
    \item Generalization can be tested by using a separate \tbf{test dataset} that the model has not seen before.
\end{itemize}

\subsubsection{Selecting Valid Loss Functions}

\begin{itemize}
    \item Assuming our examples come from a distribution $p(x,y)$, we can decompose it as $p(x,y) = p(x) \cdot p(y | x)$.
    \item The function $f(x)$ is used to predict $p(y|x)$, that is, the chance that the model will give the correct output $y$ given the input $x$.
    \item Approximating $p(y|x)$ with a function $f(x)$ is viable if we assume that the probability mass is mostly centered
    around a single point $y$ i.e. there are not multiple points $y_1, y_2, \cdots y_n$ that are likely to be be the output.
    \item However, if we move away from the previous definition of $f(x)$ and instead think of $f(x)$ as
    a parameterization of the chances of the different outputs $y_1, y_2, \cdots, y_n$ given $x$, we can represent $f(x)$ as:
    \[f(x) = [p(y_1 | x) , p(y_2 | x), p(y_3 | x)]\]
    \item Similarly, we also define $\mbf{y} \sim \operatorname{Binary}(n)$ where $\mbf{y}$ is a one-hot encoded vector that contains a 1 at the correct output's place.
    \item Thus, we can write: 
    \[p(\mbf{y} | f(x) = \prod^3_{i = 1}f_i(x)^{y_1}\]
    \item This chain of logic can be shown via an example:
    Assume there is a model, given an input $x$, outputs a $y \in \{1,2,3\}$. The model's chances of giving these outputs are, respectively, $f(x) = [0.2, 0.5, 0.3]$.
    Assume that the correct output is $y = 2$. Thus, the correct representation of $\mbf{y} \sim 3$ is $[0,1,0]$.
    Following the expression above, $p(\mbf{y} | f(x))$ can be calculated as:
    \begin{align*}
        p(\mbf{y} | f(x)) &= f_1(x)^{y_1} \cdot f_2(x)^{y_2} \cdot f_3(x)^{y_3} \\
        &= 0.2^0 \cdot 0.5^1 \cdot 0.3^0 \\
        &= 0.5 
    \end{align*}
    Thus, the probability of the model giving the correct output is 0.5.

    \item Our formula does not directly give the probability for the output, but instead gives a probability distribution over the possible outputs, with the correct one being one of them.
\end{itemize}

\subsubsection{Maximum Likelihood}

\begin{itemize}
    \item Assume that the samples are independent and identically distributed (i.i.d) from a probability distribution $p(x,y)$. Recall that the probability distribution $p(x,y)$ describes the probability of observing the input $x$ and $y$ together. Hence, the probability assigned to the dataset itself by a specific choice $f$ of function is given by the product of each sample in the dataset:
    \[p(\mathcal{S}_n|f) = \prod^{\pi}_{i=1}p(y_i|f(x_i))\]
    \item This formula defines the probability that a given model $f$ will provide outputs $y_1, y_2, \cdots, y_n$ that are the same as the outputs in the dataset $\mathcal{S}_n$ given the inputs $x_1, x_2, \cdots, x_n$.
    \item The quantity $p(\mathcal{S}_n | f)$ is called the \tbf{likelihood} of the set. This quantity is to be maximized up to a certain extent, where the model's outputs do not deviate greatly from the dataset's outputs but also where the model is also not overfitted with the dataset (and thus struggling with inputs outside of the dataset).
    \item Given a dataset $\mathcal{S}_n=\{\{x_i, y_i\}\}$ and a family of probability distributions $p(y|f(x)$ parameterized by $f(x)$, the maximum likelihood solution is given by:
    \[f^* = \arg _f \max \prod ^\pi _{i=1} p(y_i|f(x_i))\]
    Essentially, this formula provides the maximum likelihood across all models by providing the likelihood of the best function $f^*$.
    \item Switching to a minimization problem, we get:
    \[
    \arg _f \max \left\{ \log \prod ^n _{i=1} p(y_i|f(x_i))\right\} = \arg _f \min \left\{\sum^n_{i=1} -\log(p(y_i|f(x_i))\right\}
    \]
    A few notes on this equation:
    \begin{itemize}
        \item We use the log function because multiplying probabilities $<1$ can result in a very small product, leading to underflow when storing into memory. Using log helps alleviate this issue.
        \item Reminder: the log of a product is equivalent to the sum of the logs of items being multiplied:
        \[\log (ab)= \log(a) + \log(b)\]
        Converting this to an addition problem results in reduced computing demand.
        \item Adding the negative sign on the right-hand side of the equations turns this into a minimization problem which has been covered earlier.
    \end{itemize}
    \end{itemize}
    \subsection{Bayesian Learning}
    \begin{itemize}
        \item When designing a probability function $p(y|f(X))$ instead of $f(x)$, we can handle situations where the model might give different possible outputs. 
        \item This procedure however only looks an one singular function with a specific parameterization. What if we had multiple functions with different parameterization that all provide good outputs? It would be wasteful to rely on one singular function instead of relying on multiple and choosing the best model for an input based on its output.
        \item We can achieve this by defining a \tbf{prior probability distribution} $p(f)$ over all possible functions. Recall that $f$ is a model with a certain set of parameters.
        \begin{itemize}
            \item Recall that a \tbf{Bayesian prior} is a initial belief on what a parameter might be.
        \end{itemize}
        \item Functions with smaller norms are preferred so setting up an inverse relationship with the parameters' norm would be useful in creating our priors:
        \[p(f) \propto \frac{1}{||f||}\]
        \item Once a dataset is observed, the probability over $f$ shifts depending on the prior and the likelihood, and the update is given by \tbf{Bayes' theorem}.
        \[\underbrace{p(f|\mathcal{S}_n)}_{posterior}=\frac{p(\mathcal{S}_n|f))\cdot \overbrace{p(f)}^{prior}}{p(\mathcal{S}_n)}\]

        The term $p(f|\mathcal{S})$ is called the \tbf{posterior distribution function}, while the term $p(\mathcal{S}_n)$ is called the \tbf{evidence} and normalizes the right-hand side of the equation.

        \item Given an input $x$, we can make a prediction by averaging all possible models based on their posterior's weight:
        \[p(y|x) = \int _f p(y|f(x)) \cdot p(f|\mathcal{S}_n) \approx \frac{1}{k} \sum ^k _{i=1} p(y|f_i(x)) \cdot p(f_i|\mathcal{S}_n)\]
        Here, we take the average of all models when we take the summation and divide by the amount of models on the right hand side.
        \item If we are solely interested in maximizing the posterior term, we can discard the evidence term as that remains relatively constant. As a result, we can choose only to focus on the terms in the numerator:
        
        \[f^* = \arg \max p(\mathcal{S}_n | f)p(f) = \arg _f \max \left\{\underbrace{\log p(\mathcal{S}_n|f)}_{likelihood} + \underbrace{\log p(f)}_{regularization} \right\} \]
        This is the \tbf{maximum a posteriori} (MAP) solution. 
        \item If all functions have the same weight a priori, then the proble is reduced down to a maximum likelihood solution. The regularizer term pushes the solution toward the basin of attraction defined by the prior distribution.
    \end{itemize}

    \section{Linear Models}

    \subsection{Least-Squares Regression}

    \subsubsection{Problem setup}
        
    Recall that a supervised learning problem can be defined by choosing the input type $x$, the output type $y$, the model $f$, and the loss function $l$.
    
    \begin{itemize}
        \item The input is a vector $\mbf{x} \sim (c)$, corresponding to $c$ number of features in the input.
        \item The output is a single real value $\in \mathbb{R}$. 
        \begin{itemize}
            \item If $y$ can take any real value, this is a \tbf{regression} task/
            \item If $y$ can only take one out of $m$ possible values, this is a \tbf{classification} task.
            \item If $y$ can only take one of two possible values, this is a \tbf{binary classification} task.
        \end{itemize}
        \item We assume $f$ is a linear model.
        \item We assume that:
        \begin{itemize}
            \item $n \rightarrow$ size of the database
            \item $c \rightarrow$ features of input
            \item $m \rightarrow$ classes of outputs
        \end{itemize}
    \end{itemize}
    \subsubsection{Regression Losses: The Squared Loss and Variants}
    \begin{itemize}
        \item Finding the loss for regression is very simple since the prediction error $e = \{\hat{y} - y\}$. However, since we do not concern ourselves 
        with the sign of the loss, rather its absolute value, we can change the loss function to be:
        \[l(\hat{y},y) = (\hat{y}-y)^2\]
        \item The squared loss function provides many benefits, one of which is that it's rather easy to find the gradient of such a linear function of the model's input.
        \item Recalling the maximum likelihood principle, the squared loss can be obtained by assuming the outputs of the model follow a Gaussian distribution
        centered in $f(x)$ with a constance variance of $\sigma^2$.
        \[p(y|f(\mbf{x}) = \mathcal{N}(y|f(\mbf{x})), \sigma^2)\]
        Using properties of logs, we can rewrite the log-likelihood as:
        \[\log(p(y|f(\mbf{x}, \sigma^2)) = -\log(\sigma)-\frac{1}{2}\log(2 \pi) - \frac{1}{2\sigma^2}(y-f(\mbf{x}))^2\]
        \item When we minimize for $f$, the first two terms in the RHS of the equation are constant, leaving the third to be the squared loss.
        Minimizing for $\sigma^2$ is an independent operation and will be touched on later.
        \item A disadvantage to using the squared loss is that mislabelled points in the source dataset can have an undue effect on the model.
        Higher errors will be be punished with quadratically growing strenght, which lets \tbf{outliers} to negatively impact the model.
        \item Some loss functions that diminish the influence of outliers are the absolute value loss:
        \[l(\hat{y},y) = |\hat{y}-y|\]
        and the \tbf{Huber loss}, a combination of the squared loss and the absolute loss:
        \[
        L(y, \hat{y}) = 
        \begin{cases}
            \frac{1}{2}(y-\hat{y})^2 \text{ if } |y-\hat{y}| \leq 1\\
            (|y-\hat{y}|-\frac{1}{2}) \text{ otherwise }
        \end{cases}
        \]
        which is quadratic in the proximity of 0 error, and linear otherwise. The $-\frac{1}{2}$ is added for continuity, plot the equations in a 3D grapher to see why.
        \item Although the absolute value loss function might seem invalid due to its point of non-differentiability, a slight generalization of the
        derivative called the \tbf{subgradient}, can handle this point. On a practical level, gradient descent will never reach the point with
        perfect precision, so we can assume that derivatives of $|\epsilon|$ for any $\epsilon>0$ is always defined.
    \end{itemize}

    \subsubsection{The Least-Squares Model}

    \begin{itemize}
        \item A linea model on an input $\mbf{x}$ is defined as:
        \[f(\mbf{x})= \mbf{w}^{\top}\mbf{x}+b\]
        where $\mbf{w} \sim (c)$ and $b \in \mathbb{R}$ (the bias) are trainable parameters.
        \item The intuition is that given an input $\mbf{x}$ and parameters $\mbf{w}$ both holding the same number of features and parameters,
        each feature of the input will be multiplicated by its respective parameter to produce a number $\in \mathbb{R}$. This number will then be 
        added to the bias $b$.
        \item The bias term can be avoided when assuming that a 1 exists as the last feature of $\mbf{x}$:
        \begin{figure}[h]
            \centering
            \includegraphics[width=2in]{../miscImages/modifiedLinearModel.png} 
            \label{fig:example}
        \end{figure}
        \item Comibining the linear model, the squared loss, and the empirical risk minimization problem, the \tbf{least-squares regression problem} is:
        \[
        \mbf{w}^*,b^*= \arg_{w,b}\min \frac{1}{n} \sum^n_{i=1}(y_i-\mbf{w}^{\top}\mbf{x}_i-b)^2 
        \]
        \item In code, the linear model can be described as:
        \begin{lstlisting}
def linear_model (w: Float[Tensor, "c"],
        b: Float
        x: Float[Tensor, "n c"])
        -> Float[Tensor, "n"]:
    return X @ w + B
        \end{lstlisting}
        \item If we rewrite the least-squares in \tbf{vectorized} form, we can achieve optimal computing efficiency: \\
        The input in vectorized form:
        \[
        \mbf{X} = 
        \begin{bmatrix}
        \mbf{x}_1^{\top} \\
        \vdots \\
        \mbf{x}_n^{\top}
        \end{bmatrix}
        \sim (n,c)
        \]
        The output in vectorized form:
        \[
        y = 
        \begin{bmatrix}
            y_1, \cdots, y_n
        \end{bmatrix}
        ^{\top}
        \]
        The output model for a batch of values is:
        \[f(\mbf{X})=\mbf{Xw}+1b\] 
        \item A note: the ordering of the rows of the input and output do not matter the as the changes will be reflected in $f(\mbf{X})$. This concept is 
        called \tbf{permutation variance} and will be touched on later.
        \item The vectorized least-squares problem becomes:
        \[LS(\mbf{w},b)=\frac{1}{n}||\mbf{y}-\mbf{Xw}-\mbf{1}b||^2\]
    \end{itemize}
    \subsubsection{Solving the Least-Squares Problem}
    \begin{itemize}
        \item Applying differentiation to the least-squares equation, we get the gradient:
        \[\nabla LS(\mbf{w})=\mbf{X}^{\top}(\mbf{Xw-y})\]
        The global minimum can then be found by setting the gradient to 0:
        \[
            \mbf{X}^{\top}(\mbf{Xw-y}) = 0 \Rightarrow \mbf{X}^{\top}\mbf{Xw} = \mbf{X}^{\top}\mbf{y}
        \]
        Solving for the optimal parameters $\mbf{w}$ leads us to this:
        \[\mbf{w}_*=(\mbf{X}^{\top}\mbf{X})^{-1}\mbf{X^{\top}y}\]
        \item The matrix $\mbf{(X^{\top}X)}^{-1}\mbf{X^{\top}}$ is called the \tbf{pseudoinverse} of the non-square matrix $\mbf{X}$. This inversion is not 
        always possible: for example, if one features is a scalar multiple of another. 
        \item When implementing the numerical solution to this problem, we must know when to stop. We can do this by evalutating the difference between 
        two iterations for some numerical threshold $\epsilon > 0$. 
        \[||\mbf{w}_{t+1}-\mbf{w}_t||^2 < \epsilon\]
        \item An implementation of solving for the solution using gradient descent:
        \begin{lstlisting}
def least_squares_gd(X: Float[Tensor, "n c"],
                     y: Float[Tensor, "n"],
                     learning_rate=1e-3) \
                     -> Float[Tensor, "c"]:
    # Initializing the parameters
    w = torch.randn((X.shape[1], 1))
    
    # Fixed number of iterations
    for i in range(15000):
        # Note the sign: the derivative has a minus!
        w = w + learning_rate * X.T @ (y - X @ w)
    return w
        \end{lstlisting}
    \item Once training has finished, $\sigma ^2$ can also be optimized:
    \[\sigma_*^2 = \frac{1}{n}\sum^n_{i=1}(y_i-\mbf{w}_*^{\top}\mbf{x}_i)^2\]
    \end{itemize}
    \subsubsection{Some Computational Considerations}
    \begin{itemize}
        \item A matrix $\mbf{X}^{\top}X$ is well conditioned if the solution of the system remains stable and accurate regardless of small changes to the
        input. if we have an unstable system, the quality of the solution may degrade by a large amount and large numerical errors may arise.
        \item Computing the vector-product $\mbf{Xw}$ can help avoid quadratic time complexity in the equation for the gradient.
        \item This can lead to linear time complexity in both $c$ and $n$.
        \item Such optimization measures are necessary for \tbf{reverse-mode automatic differentiation} a.k.a \tbf{back-propagation}.
    \end{itemize}
    \subsubsection{Regularizing the Least-Squares Solution}
    \begin{itemize}
        \item When the matrix $\mbf{X}$ is singular, we can modify the problem to achieve a solution which is "as close as possible"
        to the original one.
        \item Adding a small multiple $\lambda > 0$ of the identity matrix to the matrix being inverted:
        \[\mbf{w}^* = (\mbf{X^{\top}X}+\lambda \mbf{I})^{-1} \mbf{X^{\top}y}\]
        Going back to the modified optimization problem:
        \[LS-Reg(\mbf{w})= \frac{2}{n}||\mbf{y-Xw}||^2+\frac{\lambda}{2}||\mbf{w}||^2\]
        \item This now becomes the regularized least-squares.
    \end{itemize}
    \subsection{Linear Models for Classification}
    \begin{itemize}
        \item Recall that $m$ defines the number of classes. To find the classification, requiring the model to return a certain integer is not a good idea.
        Instead we can regress on a real value $\tilde{y_i} \in [1, m]$. During inference, given the output $\hat{y_i}=f(\mbf{x_i})$, 
        we can map back to the original domain using:
        \[\text{Predicated class} = \operatorname{round}(\hat{y_i})\]
        \item However, this presents some disadvantages. By defining certain classifications as being close to each other i.e. class 2 is "closer" to class 3 
        than class 4, we trick the model into thinking that class 2 and class 3 are more related than class 2 and class 4. In reality, class 2 may be dog, class 3 a dolphin,    
        and class 4 a cat.
        \item A method of avoiding this problem is using a \tbf{one-hot encoded} version of $y$, which we denote by $\mbf{y}^{oh} \sim$ Binary($m$).
        \[
        \textcolor{gray}{[}\mbf{y}^{oh}\textcolor{gray}{]_j}
        \begin{cases}
            1 \text{ if } y = j \\
            0 \text{ otherwise}
        \end{cases}
        \]
        \item This notation provides some advantages in that the Euclidean distance between two classes is either 0 (same class) or $\sqrt{2}$ (different classes).
        \item This approach is not vaiable since this is discrete and we need a continiuous representation to train our dataset through gradient descent.
        \item A more better and more elegant solution exists in the form of \tbf{logistic regression}.
    \end{itemize}
    \subsubsection{The Probability Simplex and the Softmax Function}   
    \begin{itemize}
        \item The \tbf{probability simplex} $\Delta _n$ is the set of vectors $\mbf{x} \sim \Delta(n)$ such that:
        \[x_i \geq 0, \sum_i x_i=1\]
        \item Essentially, the probability simplex is the set of vectors whose elements describe the probability of the output classes and who all sum up to 1.
        Given a value $\mbf{x} \in \Delta_n$, we can project to the predicted class using:
        \[\arg_i \max{\mbf{x}_i}\]
        \item An example: let the model output the following:
        \[[0.2,0.3, 0.4]\]
        where class 0 is dog, class 1 is cat, and class 2 is bird. In this case, the maximum would be 0.4 and as such, the model would ouput bird as the output class.
        \item The previous method of linear regression assumed $y$ would be a scalar value. In this new implementation, we take $y$ to be an $m$-dimensional vector:
        \[\mbf{y = Wx+b}\]
        where $\mbf{W} \sim (m,c)$ is $m$ linear regressions models running in parallel and $b \sim (m)$.
        \item After computing $\mbf{y}$, it is not guaranteed to be in the simplex. That is why we must use a \tbf{softmax} function to project the output
        inside the simplex: \\
        For a generic vector $\mbf{x} \sim (m)$:
        \[\textcolor{gray}{[}\operatorname{softmax(\mbf{x})}\textcolor{gray}{]_i} = \frac{\operatorname{exp}(x_i)}{\sum_j \operatorname{exp}(x_j)}\]
        \item Breaking down the formula, this is what it does:
        First, exponentiation converts the numerator into a positive value.
        \[h_i=\operatorname{exp}(x_i)\]
        Next, we compute a normalization factor as the sum of exponentiated elements of the vector:
        \[Z = \sum_j h_j\]
        The output of the softmax is given by dividing $h_i$ by $Z$, ensuring all elements of the final output vector after the softmax sum to 1:
        \[y_i = \frac{h_i}{Z}\]
        \item We can also add another parameter $\tau > 0$ called the temperature:
        \[\operatorname{softmax}(\mbf{x};\tau)=\operatorname{softmax}(\mbf{x}/\tau)\]
        \item The softmax keeps the order between elements intact, but the temperature influences how far away they are from each other.
        \begin{figure}[h]
            \centering
            \includegraphics[width=4in]{../miscImages/tempSoftmax.png} 
            \caption{Example of softmax function with temperature set to 1(b), 10(c), and 100(d)}
            \label{fig:example}
        \end{figure}
        \item There are certain limiting cases:
        \begin{align*}
            \lim_{\tau \rightarrow \infty} \operatorname{softmax}(\mbf{x};\tau)= 1/c \\
            \lim_{\tau \rightarrow 0} \operatorname{softmax}(\mbf{x};\tau) = \arg _i \max \mbf{x}
        \end{align*}
    \end{itemize}
    \subsubsection{The Logistic Regression Model}
    \begin{itemize}
        \item Bringing together softmax and the parameterized output linear model, we get:
        \[\hat{y}=\operatorname{softmax}(\mbf{Wx + b})\]
        The prenomalized outputs $\mbf{h=Wx+b}$ are called the logits of the model.
        \item Covering the loss function, we can call upon previously discussed topics and derive this:
        \[p(\mbf{y}^{oh}|\mbf{\hat{y}}) = \prod_i \hat{y}_i^{y_i^{oh}}\]
        \item The \tbf{cross-entropy} loss function between $\mbf{y}^{0h}$ and $\mbf{\hat{y}}$ is given by:
        \[\operatorname{CE}(\mbf{y}^{oh}, \mbf{\hat{y}})= - \sum _i y_i^{oh} \log(\hat{y}_i)\]
        The cross-entropy loss function is widely used for classification tasks, especially in multi-class problems. 
        It measures the dissimilarity between the true probability distribution (actual labels) and the predicted probability distribution (model's predictions).
        The goal of cross-entropy is to quantify how well the predicted probabilities match the actual labels.
        \item Given that only one value of $\mbf{y}^{oh}$ will be non-zero corresponding to the true class $y = \arg _i \max \{y_i^{oh}\}$, we can simplify the
        loss as:
        \[\operatorname{CE}(y,\mbf{\hat{y}})=- \log (\hat{y}_y)\]
        \item It is evident that minimizing CE loss is equivalent to maximizing the output probability corresponding to the true class. Of course, the presence 
        of the softmax function implies that an increase in the probabilithy of one ouput leads to a decrease in the probability of all other outputs. Putting everything together,
        we get the \tbf{logistic regression optimization problem}.
        \[\operatorname{LR}(\mbf{W,b}) = \frac{1}{n}\sum_{i=1}^n\operatorname{CE}(\mbf{y}_i^{oh}, \operatorname{softmax}(\mbf{Wx}_i+\mbf{b}))\]
    \end{itemize}
    \subsection{Additional Topics on Classification}
    \subsubsection{Binary Classification}
    \begin{itemize}
        \item With the special case of $m=2$, we have the problem of \tbf{binary classification} also known as \tbf{concept learning}.
        \item Normally, the function would return an output made up of two elements, however this is unnecessary considering the softmax function 
        ensures all probabilities sum up to 1. As such, we can rely on the function producing one output $\in$ [0,1] and derive the classification as:
        \[
        \text{Predicted class} = \operatorname{round}(f(\mbf{x})) = 
        \begin{cases}
            0 \text{ if } f(\mbf{x}) \leq 0.5 \\
            1 \text{ otherwise }
        \end{cases}
        \]
        \item To achieve the normalization, the first output of a two-valued softmax can be rewritten as:
        \[\frac{\operatorname{exp}(x_1)}{1+\operatorname{exp}(x_1)}\]
        Note: we assume $x_0$ to be 0, hence $e^0 = 1$ is in the denominator.
        \item Simplifying by dividing by $\exp(x_1)$, we get the \tbf{sigmoid} function $\sigma(s) : \mathbb{R} \rightarrow [0,1]$ function:
        \[\sigma(s) = \frac{1}{1+\exp(-s)}\]
        The sigmoid maps any real number to a number between 0 and 1.
        \item The \tbf{binary logistic regression} model is obtained by combining a one-dimensional linear model with a sigmoid scaling of the output:
        \[f(\mbf{x})= \sigma(\mbf{w^{\top}x}+b)\]
        \item The cross-entropy simplifies down to:
        \[\operatorname{CE}(\hat{y},y)=\underbrace{-y\log(\hat{y})}_{\text{class 1 loss}}\underbrace{-(1-y)\log(1-\hat{y})}_{\text{class 2 loss}}\]
        \item In the binary classification case, we can solve the problem with two equivalent approaches: (a) a two-valued model with standard softmax,
        or (b) a simplified one-valued output with a sigmoid output transformation.
        \item The gradient of the binary logistic regression model with respect to $\mbf{w}$ is:
        \[\nabla \operatorname{CE}(f(\mbf{x}),y)=(f(\mbf{x})-y)\mbf{x}\]
        which is similar to the linear least-squares regression:
        \[\nabla \operatorname{LS}(\mbf{w})=\mbf{X^{\top}(Xw-y)}\]
        \item We may also rewrite our model as:
        \[\underbrace{\mbf{w^{\top}x}+b}_{\text{logits}}=\underbrace{\log(\frac{y}{1-y})}_{\sigma^{-1}(y)}\]
    \end{itemize}
    \subsubsection{The logsumexp Trick}
    \begin{itemize}
        \item Consider the $i$-th term of the cross entropy in terms of the logits $\mbf{p}$:
        \[-\log(\frac{\exp p_i}{\sum_j \exp p_j})\]
        Due to the unknown magnitude of the logits, the exponentiation of the logits can cause numerical computing errors.
        \item To solve, we can rewrite as:
        \[-p_i+\log(\sum_j \exp p_j)\]
        The first term does not suffer from instabilities, and the second term can be taken care of by using a scalar sum/difference 
        $c$ which won't change the final result when added back in:
        \[\operatorname{logsumexp}(\mbf{p})=\operatorname{logsumexp}(\mbf{p}-c)+c\]
    \end{itemize}
    \subsubsection{Calibration and Classification}
    \begin{itemize}
        \item We must highlight a key difference in the conclusions we get from a model's output. \\
        The following sentence is justified:
        \begin{quotation}
            \say{The predicted class of $f(\mbf{x})$ is $\arg _i \max \textcolor{gray}{[}f(\mbf{x})\textcolor{gray}{]_i}$.}
        \end{quotation}
        But this one is not:
        \begin{quotation}
            \say{The probability of $\mbf{x}$ being of class $i$ is \textcolor{gray}{[}$f(\mbf{x})\textcolor{gray}{]_i}$.}
        \end{quotation}
        \item When the confidence scores of the network match the probability of a given prediction being correct,
        the network's outputs are \tbf{calibrated}.
        \item Different predictions may lead to different costs, and a \emph{cost matrix} assigning a cost $C_{ij}$ for 
        any input of class $i$ predicted as class $j$. An example is shown below:
        \begin{figure}[h]
            \centering
            \includegraphics[width=2in]{../miscImages/costMatrixExample.png} 
            \caption{Example of cost matrix for a classification problem having assymetric costs of missclassification}
            \label{fig:example}
        \end{figure}
        This may very well be the cost matrix for a classification model used in medical diagnosis. A false negative may be much more harmful than a false positive.
        \item A cost matrix $C \sim (m,m)$ can be created for a multiclass problem and used to minize the expected cost assigned by our model:
        \[\arg _i \min \sum^m_{j=1}C_{ij}\textcolor{gray}{[}f(\mbf{x})\textcolor{gray}{]_j}\]
        Essentially, you want to predict something by minimizing the combination of its original probability with the associated costs of choosing it.
    \end{itemize}
    \subsubsection{Estimating the Calibration Error}
    \begin{itemize}
        \item To estimate the calibration of a model, we can bin the predictions like this:
        \begin{itemize}
            \item Split the interval [0.1] into $b$ equispaced bins such that each bin is of size $1/b$. 
            \item Take a validation dataset of size $n$ and denote by $\mathcal{B}_i$, the elements whose confidence falls into bin $i$.
            \item For each bin, we can calculate the average confidence $p_i$ of the model and the average accuracy $a_i$.
        \end{itemize}
        \item To have a single scalar metric of calibration, we can use the \tbf{expected calibration error}:
        \[\text{ECE} = \sum_i \underbrace{\frac{|\mathcal{B}_i|}{n}}_{\text{bin $i$}}\overbrace{|a_i-p_i|}^{calibration}\]
        The calibration is intended to punish instances where accuracy and confidence are not aligned together (low confidence/high accuracy \& vice versa)
        \item A low expected calibration error suggests that the prediction confidence is aligned with the accuracy of the prediction.
        \item If the model is found to be calibrated, the model will need to be modified. We can do this by rescaling the predictions
        via temperature scaling or optimizing with a different loss function.
        \item As opposed to direct calibration of a model, we can do \tbf{conformal prediction}. Take a threshold probability $\gamma$.
        We can take the set of classes predicted by the modle whose corresponding probability is higher than $\gamma$.  
        \[\mathcal{G}(\mathbf{x})=\{i \mid \textcolor{gray}{[}f(\mbf{x})\textcolor{gray}{]_i} > \gamma\}\]
        Now the set $\mathcal{G}(\mbf{x})$ is the set of potential classes.
        \item Essentially, we're defining a bare minimum level of probability of choosing the correct class. The minimum is $\gamma$, 
        and the acceptable error is $1 - \gamma = \alpha$. 
    \end{itemize}
    \section{Fully-Connected Models}
    \subsection{The Limitations of Linear Models}
    \begin{itemize}
        \item As a review, linear models work by multiplying each feature of the input with a weight, then adding a bias to get the complete result.
        \item Linear models do not work well when features are not independent of each other.
        \item For example, two clients of a bank who are identical in all aspects except for their income, with $\mbf{x'}$ having double the income of 
        of $\mbf{x}$. If $f$ is a linear model with no bias, we have:
        \[
f(\mathbf{x}') = \overbrace{f(\mathbf{x})}^{\text{\scriptsize Original}} + \underbrace{w_j x_j}_{\mathclap{\text{\scriptsize Change induced by } \mathbf{x}_j = 2x_j}}\]
        \item A consequence is that the change in input reflects a similar sized change in the output. This will not work for relationships where features can affect other features or certain combinations of features have different outputs.
        Take for example this scenario where we're trying to score the users:
        \emph{an income of 1500 is low, except if the age $<$ 30}.
    \end{itemize}

    \subsection{Composition and Hidden Layers}
    \begin{itemize}
        \item Imagining our model $f$ is a composition of two trainable operations results in : 
        \[f(\mbf{x}) = (f_2 \odot f_1)(\mbf{x}) = f_2(f_1(\mbf{x}))\]
        Being more specific, we describe the relation as:
        \begin{align*}
            \mbf{h} = f_1(\mbf{x}) = \mbf{W}_1\mbf{x} + \mbf{b}_1 \\
            y = f_2(\mbf{h}) = \mbf{w}_2^\top \mbf{h} + b_2
        \end{align*}
        But doing this will result in the two projections collapsing into a single one:
        \[y = (\underbrace{\mbf{w_2 ^\top \mbf{W}_1}}_{\mbf{A}}) \mbf{x} 
        + (\underbrace{\mbf{w}_2 ^\top \mbf{b}_1 + b_2}_{\mbf{c}}) 
        = \mbf{Ax+c}\]
        \item The idea behind \tbf{fully-connected} (FC) models, also known as \tbf{multi-layer perceptrons} (MLPs), is to insert a 
        simple elementwise non-linearity $\phi: \mathbb{R} \rightarrow \mathbb{R}$ in-between projections to avoid the collapse:
        \[\mbf{h} = f_1(\mbf{x}) = \phi(\mbf{W_1x + b_1})\]
        \[y = f_2(\mbf{h}) = \mbf{w_2 ^\top h} + b_2\]
        The function $\phi$ can be non-linearity such as a polynomial, a square-root, or the sigmoid function $\sigma$. 
        A good choice for the function is something which is non-linear enough to prevent collapse, but linear enough to preserve the original identity in its derivative. \\
    
        A good default choice is the \tbf{rectified linear unit} (ReLU)
        \item The \tbf{rectified linear unit} is defined elementwise as:
        \[\operatorname{ReLU}(s) = \operatorname{max}(0,s)\]
        \item With the addition of $\phi$, we can chain as many transformations as we want:
        \[y = \mbf{w}_l ^\top \phi (\mbf{W} _{l - 1}(\phi (\mbf{W}_{l-2} \phi (\hdots) + \mbf{b}_{l-2}))+\mbf{b}_{l-1})+b_l\]
        \end{itemize}
        
        \section*{\normalsize On the Terminology in Differentiable Models}
        This will serve as an summary of terminology.
        \begin{itemize}
            \item Each $f_i$ is called a \tbf{layer} of the model, with $f_l$ being the \tbf{output layer}, $f_i, i = 1, \hdots, l - 1$ the \tbf{hidden layers}.
            \item With some notation overloading, $\mbf{x}$ is the input layer.
            \item With this terminology, we can restate the definition of the \tbf{fully-connected layer} in batched form:
            \item For a batch of $n$ vectors, each of size $c$, represented as a matrix $\mbf{X} \sim (n, c)$, a \tbf{fully-connected} (FC) layer is defined as:
            \[\operatorname{FC}(\mbf{X}) = \phi (\mbf{XW + b})\]
            \item The parameters of the layer are the matrix $\mbf{W} \sim (c',c)$ and the bias vector $\mbf{b} \sim (c')$, for a total of $(c' + 1)c$ parameters (assuming $\phi$ does not have any parameters).
            Its hyper-parameters are the width $c'$ and the non-linearity $\phi$. 

            \tbf{Reminder:} A hyperparameter is one set by the user to help define the behavior of the model. 

            \tbf{Note:} The above definition is one from the book and may contain an error. The expression $\mbf{XW}$ is not valid if $\mbf{X}$ is of dimensions $n \times c$
            and $\mbf{W}$ is of dimensions $c' \times c$. The no. of columns in the first matrix does not match the no. of rows in the second. The product $\mbf{XW}$ should be of dimensions $n \times c'$.

            \tbf{Note:} The addition of the product $\mbf{XW}$ and the bias $\mbf{b}$ might be seen as invalid due to dimension mismatches, however ML frameworks can extend the bias' dimensions to allow addition to occur. 
            So the result of $\mbf{XW} + \mbf{b}$ would have the dimensions $n \times c'$.

            \item The outputs $f_i(\mbf{X})$ are called the \tbf{activations} of the layer. We can also differentiate between the \tbf{pre-activation} and the \tbf{post-activation} (before and after the non-linearity has been applied).
            \item An implementation of a fully-connected layer using PyTorch:
            \begin{lstlisting}
import torch
import torch.nn as nn
from torch.nn.functional import relu

class FullyConnectedLayer(nn.Module):
    def __init__(self, c: int, cprime: int):
        super().__init__()
        # Initialize the parameters
        self.W = nn.Parameter(torch.randn(c, cprime))
        self.b = nn.Parameter(torch.randn(1, cprime))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return relu(x @ self.W + self.b)
            \end{lstlisting}
            
            \item The non-linearity $\phi$ can be called the \tbf{activation function} and each output of $f_i$ is called a \tbf{neuron}. The width (shape of the output) of each layer is a hyperparameter that can be set by the user.
            The width only affects the input going into the next layer.
        \end{itemize}
        \subsubsection{Approximation Properties of MLPs}
        \begin{itemize}
            \item Training MLPs is similar to training linear models where we minimize a loss function.
            \item Linear models has one global minimum that gradient descent would be guided toward. However, given the non-linearity of multiple layers, there can exist 
            many local minima with no guranatee that gradient descent will the find the global minimum. Regardless, research has shown that non-convex models can still provide good performance.
            \item The question naturally arises, if linear models can only solve tasks which are linearly separable, what class of tasks can non-linear models be used for? 
            
            Having a single hidden layer is enough to have \tbf{universal approximation} capabilities.
            \item \tbf{Universal approximatino of MLPs:} Given a continuous function $f : \mathbb{R}^d \rightarrow \mathbb{R}$, we can always find a model $f(\mbf{x})$ that is a MLP with a single hidden layer such that:
            \[|f(\mbf{x}) - g(\mbf{x})| \leq \epsilon, \forall \mbf{x}\]
            \item In english, what this means is that given a function, we can find a model that can approximate that function with a certain degree of acceptable error $\epsilon$. 
        \end{itemize}
        \subsection{Stochastic Optimization}
        \begin{itemize}
            \item When $n$ (the size of the dataset) grows very large, preserving the linear time complexity becomes unreasonable especially when $n$ is in the order of 
            $10^4$ or more.
            \item A solution to this problem is to use subsets of data to computer descent direction. To this end, for iteration $t$ of gradient descent we sample a subset $\mathcal{B}_t \subset \mathcal{S}_n$ of $r$ points (with $r \ll n$).
            Shuffling the dataset, splitting into batches, then training with each subset of the data, and finally reshuffling the dataset before training again. 
            \begin{figure}[h]
                \centering
                \includegraphics[width=6in]{../miscImages/stochasticOptimization.png} 
                \caption{Building the mini-batch sequence}
                \label{fig:example}
            \end{figure}
            \item Computing the approximated loss only considering the minibatch as:
            \[\overset{\sim}{L}_t = \underbrace{\frac{1}{r} \sum _{(x_i, y_i) \in \mathcal{B}_t}}_{\text{Mini-batch}}l(y_i, f(x_i)) \approx \underbrace{\frac{1}{n} \sum _{(x_i, y_i) \in \mathcal{S}_n}}_{\text{Full dataset}} l(y_i, f(x_i))\]
            \item For the computation of the gradient and the loss for a batch, the complexity grows with $r$. Having lower values of $r$ results in higher gradient variance, while higher $r$ results in slower and mroe precise iterations.
            \item Gradient descent applied on mini-batches of data is called \tbf{stochastic gradient descent} (SGD). 
            \item How can we create the mini-batches? Sampling random datapoints can be computationally expensive with large datasets, so another approach is needed. We can begin by shuffling the dataset, selecting batches of consecutive elements, training with them, then shuffling again.
            Assuming a database of size $n = br$, we will have $b$ batches of data with $r$ elements in them.
            \item One complete loop of the shuffle/batch/train cycle is called an \tbf{epoch} of training. The amount of epochs in the training stage is a hyperparameter that can be set by the user. It is given by $\frac{n}{b}$. Setting the batch size will set the number of epochs.
        \end{itemize}
        \subsection{Activation Functions}
        \begin{itemize}
            \item Regarding the selection of the non-linearity funciton, any element-wise non-linearity function is valid, but not all of them have good performance or numerical stability.
            For example, the non-linearity function
            \[\phi(s) = s^p \]
            does preserve linearity but with increasing quantities of $s$, the function becomes computationally expensive and numerically unstable.
            \item Looking through the frame of neurology, the weights $\mbf{w} ^\top$ in the dot product $\mbf{w}^\top \mbf{x}$ were simple models of synapses, the bias $b$ was the threshold, and the neuron was activated when the sum of the inputs surpased the threshold.
            \[s = \mbf{w}^\top \mbf{x} - b, \phi(s) = \mathbb{I}_{s \geq 0}\]
            where $\mathbb{I}_b$ is an indicator funciton that is 1 if $b$ is true, else 0.
            

        \end{itemize}
\end{document}